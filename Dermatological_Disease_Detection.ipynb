{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdHcl07sw/aV2lkmnK5ONL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NafishaNower/Dermatological-Disease-Detection-Using-Image-Processing-And-Deep-Learning/blob/main/Dermatological_Disease_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unet"
      ],
      "metadata": {
        "id": "byFHogNjLpb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLyKfJbQKjnA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Assuming you have 7 types of diseases\n",
        "num_classes = 7\n",
        "\n",
        "# Set the paths to your image and mask folders\n",
        "image_folder = \"/content/drive/MyDrive/final 438/disease/Original/Final dataset\"\n",
        "mask_folder = \"/content/drive/MyDrive/final 438/disease/Mask/Mask\"\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "image_filenames = os.listdir(image_folder)\n",
        "mask_filenames = os.listdir(mask_folder)\n",
        "\n",
        "image_filenames.sort()\n",
        "mask_filenames.sort()\n",
        "\n",
        "images = []\n",
        "masks = []\n",
        "\n",
        "for img_filename, mask_filename in zip(image_filenames, mask_filenames):\n",
        "    img_path = os.path.join(image_folder, img_filename)\n",
        "    mask_path = os.path.join(mask_folder, mask_filename)\n",
        "\n",
        "    img = load_img(img_path, target_size=(128, 128))\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    images.append(img_array)\n",
        "\n",
        "    # Load and preprocess the mask (assuming it's one-hot encoded)\n",
        "    mask = load_img(mask_path, target_size=(128, 128), color_mode='grayscale')\n",
        "    mask_array = img_to_array(mask) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    masks.append(mask_array)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Step 2: Build the Modified U-Net Model for 7 Classes\n",
        "def unet_model(input_size=(128, 128, 3), num_classes=num_classes):\n",
        "    inputs = tf.keras.Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    # Decoder\n",
        "    up6 = layers.Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)\n",
        "    up6 = layers.concatenate([up6, conv4], axis=-1)\n",
        "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n",
        "    up7 = layers.concatenate([up7, conv3], axis=-1)\n",
        "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n",
        "    up8 = layers.concatenate([up8, conv2], axis=-1)\n",
        "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(up8)\n",
        "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n",
        "    up9 = layers.concatenate([up9, conv1], axis=-1)\n",
        "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(up9)\n",
        "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
        "\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create an instance of the U-Net model\n",
        "model = unet_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Training\n",
        "history = model.fit(X_train, y_train_categorical, epochs=10, validation_data=(X_test, y_test_categorical))\n",
        "\n",
        "# Step 4: Evaluation\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Save the model architecture as JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"unet_skin_disease_classification_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save the weights\n",
        "model.save_weights(\"unet_skin_disease_classification_weights.h5\")\n",
        "\n",
        "# Save the entire model (architecture + weights)\n",
        "model.save(\"unet_skin_disease_classification_model_complete.h5\")\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy curves\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ... (Previous code remains unchanged)\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy Curves')\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test_categorical, axis=1), np.argmax(y_pred, axis=1))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(np.argmax(y_test_categorical, axis=1), np.argmax(y_pred, axis=1)))\n",
        "\n",
        "# Visualize CNN Model Architecture\n",
        "tf.keras.utils.plot_model(model, to_file='unet_skin_disease_classification_model.png', show_shapes=True)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(np.argmax(y_test_categorical, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "recall = recall_score(np.argmax(y_test_categorical, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "f1 = f1_score(np.argmax(y_test_categorical, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(\"model_diagram.png\")\n",
        "\n",
        "# Visualize CNN Model Architecture\n",
        "tf.keras.utils.plot_model(model, to_file='unet_skin_disease_classification_model.png', show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG19"
      ],
      "metadata": {
        "id": "IaZWWelbLv0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from skimage import exposure  # Add this line for the exposure module\n",
        "\n",
        "# Assuming you have 7 types of diseases\n",
        "num_classes = 7\n",
        "\n",
        "# Set the paths to your image and mask folders\n",
        "image_folder = \"/content/drive/MyDrive/final 438/disease/Original/Final dataset\"\n",
        "mask_folder = \"/content/drive/MyDrive/final 438/disease/Mask/Mask\"\n",
        "\n",
        "# ... (Previous data loading and preprocessing code remains unchanged) ...\n",
        "# Step 1: Load and Preprocess Data\n",
        "image_filenames = os.listdir(image_folder)\n",
        "mask_filenames = os.listdir(mask_folder)\n",
        "\n",
        "image_filenames.sort()\n",
        "mask_filenames.sort()\n",
        "\n",
        "images = []\n",
        "masks = []\n",
        "labels = []\n",
        "\n",
        "for img_filename, mask_filename in zip(image_filenames, mask_filenames):\n",
        "    img_path = os.path.join(image_folder, img_filename)\n",
        "    mask_path = os.path.join(mask_folder, mask_filename)\n",
        "\n",
        "    # Load image and apply histogram equalization\n",
        "    img = load_img(img_path, target_size=(224, 224), color_mode='grayscale')\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    img_array = exposure.equalize_hist(img_array.squeeze())  # Apply histogram equalization\n",
        "    img_array = np.expand_dims(img_array, axis=-1)  # Add back the channel dimension\n",
        "    img_array_rgb = np.concatenate([img_array, img_array, img_array], axis=-1)  # Create pseudo-RGB image\n",
        "    images.append(img_array_rgb)\n",
        "\n",
        "    # Extract label from the filename\n",
        "    label_parts = img_filename.split('-')[-1].split('.')[0].split('_')\n",
        "    label = \"\".join(label_parts[:-1]).replace(\" \", \"\")\n",
        "    labels.append(label)\n",
        "\n",
        "    # Load and preprocess the mask (assuming it's one-hot encoded)\n",
        "    mask = load_img(mask_path, target_size=(224, 224), color_mode='grayscale')\n",
        "    mask_array = img_to_array(mask) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    masks.append(mask_array)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_categorical = tf.keras.utils.to_categorical(label_encoder.fit_transform(labels), num_classes)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Rest of the code up to the model creation remains unchanged...\n",
        "\n",
        "# Step 2: Build the Modified VGG19 Model for 7 Classes\n",
        "def vgg19_model(input_size=(224, 224, 3), num_classes=num_classes):\n",
        "    base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=input_size)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create an instance of the VGG19 model\n",
        "model = vgg19_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ... (Callbacks and Training code remains unchanged) ...\n",
        "# Learning Rate Scheduler\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.9 ** epoch)\n",
        "# Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Step 3: Training\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[lr_scheduler, early_stopping])\n",
        "\n",
        "# Plot training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy Curves')\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = model.predict(X_test)\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "recall = recall_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n",
        "\n",
        "# Visualize CNN Model Architecture\n",
        "tf.keras.utils.plot_model(model, to_file='vgg19_skin_disease_classification_model.png', show_shapes=True)\n"
      ],
      "metadata": {
        "id": "oFhTszelLvMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileNetV2"
      ],
      "metadata": {
        "id": "i_GKyIFNL2Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have 7 types of diseases\n",
        "num_classes = 7\n",
        "\n",
        "# Set the paths to your image and mask folders\n",
        "image_folder = \"/content/drive/MyDrive/final 438/disease/Original/Final dataset\"\n",
        "mask_folder = \"/content/drive/MyDrive/final 438/disease/Mask/Mask\"\n",
        "\n",
        "# Step 1: Load and Preprocess Data\n",
        "image_filenames = os.listdir(image_folder)\n",
        "mask_filenames = os.listdir(mask_folder)\n",
        "\n",
        "image_filenames.sort()\n",
        "mask_filenames.sort()\n",
        "\n",
        "images = []\n",
        "masks = []\n",
        "labels = []\n",
        "\n",
        "for img_filename, mask_filename in zip(image_filenames, mask_filenames):\n",
        "    img_path = os.path.join(image_folder, img_filename)\n",
        "    mask_path = os.path.join(mask_folder, mask_filename)\n",
        "\n",
        "    img = load_img(img_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    images.append(img_array)\n",
        "\n",
        "    # Extract label from the filename\n",
        "    label_parts = img_filename.split('-')[-1].split('.')[0].split('_')\n",
        "    label = \"\".join(label_parts[:-1]).replace(\" \", \"\")\n",
        "    labels.append(label)\n",
        "\n",
        "    # Load and preprocess the mask (assuming it's one-hot encoded)\n",
        "    mask = load_img(mask_path, target_size=(224, 224), color_mode='grayscale')\n",
        "    mask_array = img_to_array(mask) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    masks.append(mask_array)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_categorical = tf.keras.utils.to_categorical(label_encoder.fit_transform(labels), num_classes)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Build the Modified MobileNetV2 Model for 7 Classes\n",
        "base_model = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save the model architecture diagram to a file\n",
        "plot_model(model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Step 3: Training\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 4: Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Save the trained model\n",
        "model.save('mobilenetv2_skin_disease_classification_model.h5')\n",
        "\n",
        "# Plotting Loss and Accuracy Curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# F1 Score Calculation and Curve\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_scores = []\n",
        "for epoch in range(len(history.history['loss'])):\n",
        "    f1 = f1_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1), average='weighted')\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "plt.plot(f1_scores, label='Weighted F1 Score')\n",
        "plt.title('F1 Score Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# AUC-ROC Curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('AUC-ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Grad-CAM Heatmap\n",
        "def get_gradcam_heatmap(model, img_array, class_index, layer_name='out_relu'):\n",
        "    # Define a model that maps the input image to the activations of the specified layer and output layer\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Get the convolutional layer output and model prediction for the given input\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_output, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, class_index]\n",
        "\n",
        "    # Calculate gradients of the loss with respect to the output feature map of the selected layer\n",
        "    grads = tape.gradient(loss, conv_output)[0]\n",
        "\n",
        "    # Global average pooling of gradients\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    #\n"
      ],
      "metadata": {
        "id": "hBdtHOqfL9Au"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}